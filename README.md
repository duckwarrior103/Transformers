# Transformers
Includes implementation of Transformers and with various attention variants, with experiments conducted. 

## Currently Includes:
- A complete Transformer implemented with PyTorch, consisting of single head self-attention layer and a MLP
- An experiment on sorting sequence of integers with length n and vocab size v.

## To be added soon: 
- Other attention variants
- Multi-head attention
- Custom implemented light-weight auto-regressive model 
