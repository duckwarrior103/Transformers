# Transformers
Includes implementation of Transformers and with various attention variants, with experiments conducted. 

## Currently Includes:
- A complete Transformer implemented with PyTorch, consisting of single head self-attention layer and a MLP
- An experiment on sorting sequence of integers with length n and vocab size v.

## To be added soon: 
- Multi-head attention
- Multiple Transformer blocks
- Experimentation with other tasks
- Transformers with other novel types of attention mechanisms
